{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentimix with XLM-Roberta.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPVwmvuNfSrcm4Hr6NXwiAz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/graviraja/100-Days-of-NLP/blob/applications%2Fclassification/applications/classification/sentiment_classification/Sentimix%20with%20XLM-Roberta.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9J6Edrs7CEt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "49d2bf0f-03c8-4d89-e9cc-00b89a32d562"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AJskHt07XSJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_file = '/content/drive/My Drive/train_14k_split_conll.txt'\n",
        "test_file = '/content/drive/My Drive/dev_3k_split_conll.txt'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQJnhkw05Y4s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install indic_transliteration -q\n",
        "!pip install contractions -q\n",
        "!pip install transformers -q"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOYYtJ7_7bkx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "8fec4b64-ad1f-4bfd-c524-605f2117cbf3"
      },
      "source": [
        "import re\n",
        "import time\n",
        "import string\n",
        "import contractions\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from indic_transliteration import sanscript\n",
        "from indic_transliteration.sanscript import SchemeMap, SCHEMES, transliterate\n",
        "\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "from torchtext.data import Field, LabelField, TabularDataset, BucketIterator\n",
        "from transformers import XLMRobertaTokenizer, XLMRobertaModel, AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3q0e0pvKjbZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oya12Zay7bIB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(train_file) as f:\n",
        "    data = f.readlines()\n",
        "\n",
        "with open(test_file, 'r') as f:\n",
        "    test_data = f.readlines()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ko5rnZqI8RnM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parse_data(data):\n",
        "    uids, sentences, sentences_info, sentiment = [], [], [], []\n",
        "    \n",
        "    single_sentence, single_sentence_info = [], []\n",
        "    sent = \"\"\n",
        "    uid = 0\n",
        "\n",
        "    for idx, each_line in enumerate(data):\n",
        "        line = each_line.strip()\n",
        "        tokens = line.split('\\t')\n",
        "        num_tokens = len(tokens)\n",
        "        if num_tokens == 2:\n",
        "            # add the word\n",
        "            single_sentence.append(tokens[0])\n",
        "            # add the word info(lang)\n",
        "            single_sentence_info.append(tokens[1])\n",
        "        elif num_tokens == 3 and idx > 0:\n",
        "            # append the sentence data\n",
        "            sentences.append(single_sentence)\n",
        "            sentences_info.append(single_sentence_info)\n",
        "            sentiment.append(sent)\n",
        "            uids.append(uid)\n",
        "            sent = tokens[-1]\n",
        "            uid = int(tokens[1])\n",
        "            # clear the single sentence\n",
        "            single_sentence = []\n",
        "            single_sentence_info = []\n",
        "        # new line after the sentence\n",
        "        elif num_tokens == 1:\n",
        "            continue\n",
        "        else:\n",
        "            sent = tokens[-1]\n",
        "            uid = int(tokens[1])\n",
        "\n",
        "    # for the last sentence\n",
        "    if len(single_sentence) > 0:\n",
        "        sentences.append(single_sentence)\n",
        "        sentences_info.append(single_sentence_info)\n",
        "        sentiment.append(sent)\n",
        "        uids.append(uid)\n",
        "        \n",
        "    assert len(sentences) == len(sentences_info) == len(sentiment) == len(uids)\n",
        "    return sentences, sentences_info, sentiment, uids"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBmoEX8a8Uib",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences, sentences_info, sentiment, uids = parse_data(data)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdrErRrz8YPN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_sentences, test_sentences_info, test_sentiment, test_uids = parse_data(test_data)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPv5_hcW8cKK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "outputId": "8913b1e9-abb9-4fef-9f57-32f45d91adb1"
      },
      "source": [
        "list(zip(sentences[0], sentences_info[0]))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('nen', 'Eng'),\n",
              " ('√°', 'O'),\n",
              " ('vist', 'Eng'),\n",
              " ('bolest', 'Eng'),\n",
              " ('vztek', 'Eng'),\n",
              " ('smutek', 'Eng'),\n",
              " ('zmatek', 'Hin'),\n",
              " ('osam', 'Hin'),\n",
              " ('ƒõ', 'O'),\n",
              " ('lost', 'Eng'),\n",
              " ('beznad', 'Eng'),\n",
              " ('ƒõ', 'O'),\n",
              " ('j', 'Hin'),\n",
              " ('a', 'Eng'),\n",
              " ('nakonec', 'Eng'),\n",
              " ('jen', 'Hin'),\n",
              " ('klid', 'Hin'),\n",
              " ('Asi', 'Hin'),\n",
              " ('takhle', 'Hin'),\n",
              " ('vypad', 'Hin'),\n",
              " ('√°', 'O'),\n",
              " ('m', 'Hin'),\n",
              " ('≈Ø', 'O'),\n",
              " ('j', 'Eng'),\n",
              " ('life', 'Eng'),\n",
              " ('...', 'O')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ELyVIys5kY6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1ed20a28-0de4-4e02-9047-fbdf0d80993e"
      },
      "source": [
        "data = \"jen klid takhle vypad\"\n",
        "transliterate(data, sanscript.ITRANS, sanscript.DEVANAGARI)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'‡§ú‡•á‡§®‡•ç ‡§ï‡•ç‡§≤‡§ø‡§¶‡•ç ‡§§‡§ñ‡•ç‡§≤‡•á ‡§µ‡•ç‡§Ø‡•ç‡§™‡§¶‡•ç'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mf95GioQ8ydO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(sentences, sentences_info):\n",
        "    translated = []\n",
        "\n",
        "    for sent, sent_info in zip(sentences, sentences_info):\n",
        "        partial_translated = []\n",
        "        for word, word_info in zip(sent, sent_info):\n",
        "            if word_info == \"Hin\":\n",
        "                partial_translated.append(transliterate(word, sanscript.ITRANS, sanscript.DEVANAGARI))\n",
        "            else:\n",
        "                partial_translated.append(word)\n",
        "        translated.append(partial_translated)\n",
        "    \n",
        "    return translated"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzxmcuX5-5F_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "translated_sentences = translate(sentences, sentences_info)\n",
        "test_translated_sentences = translate(test_sentences, test_sentences_info)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXpDjYLU_ApQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url_pattern = r'https(.*)/\\s[\\w\\u0900-\\u097F]+'\n",
        "special_chars = r'[_‚Ä¶\\*\\[\\]\\(\\)&‚Äú]'\n",
        "names_with_numbers = r'([A-Za-z\\u0900-\\u097F]+)\\d{3,}'\n",
        "apostee = r\"([\\w]+)\\s'\\s([\\w]+)\"\n",
        "names = r\"@[\\s]*[\\w\\u0900-\\u097F]+[\\s]*[_]+[\\s]*[\\w\\u0900-\\u097F]+|@[\\s]*[\\w\\u0900-\\u097F]+\"\n",
        "\n",
        "def preprocess_data(sentence_tokens):\n",
        "    sentence = \" \".join(sentence_tokens)\n",
        "    sentence = \" \" + sentence\n",
        "    # remove rt and ‚Ä¶ from string\n",
        "    sentence = sentence.replace(\" RT \", \"\")\n",
        "    sentence = sentence.replace(\"‚Ä¶\", \"\")\n",
        "    # replace apostee\n",
        "    sentence = sentence.replace(\"‚Äô\", \"'\")\n",
        "    # replace names\n",
        "    sentence = re.sub(re.compile(names), \" \", sentence)\n",
        "    # remove urls\n",
        "    sentence = re.sub(re.compile(url_pattern), \"\", sentence)\n",
        "    # combine only ' related words => ... it ' s ... -> ... it's ...\n",
        "    sentence = re.sub(re.compile(apostee), r\"\\1'\\2\", sentence)\n",
        "    # fix contractions\n",
        "    sentence = contractions.fix(sentence)\n",
        "    # replace names ending with numbers with only names (remove numbers)\n",
        "    sentence = re.sub(re.compile(names_with_numbers), r\" \", sentence)\n",
        "    sentence = \" \".join(sentence.split()).strip()\n",
        "    return sentence\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXD80DU8qT2C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MODEL_NAME = \"xlm-roberta-base\"\n",
        "tokenizer = XLMRobertaTokenizer.from_pretrained(MODEL_NAME)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCzRR0dAq4vz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "b6d45b64-b668-4ece-bcba-d0c86f44874b"
      },
      "source": [
        "print(tokenizer.sep_token, tokenizer.sep_token_id)\n",
        "print(tokenizer.cls_token, tokenizer.cls_token_id)\n",
        "print(tokenizer.pad_token, tokenizer.pad_token_id)\n",
        "print(tokenizer.unk_token, tokenizer.unk_token_id)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "</s> 2\n",
            "<s> 0\n",
            "<pad> 1\n",
            "<unk> 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSzkD2Tw_58j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "09288980-e49e-48cd-9a2e-94c6f4af2a1c"
      },
      "source": [
        "\" \".join(sentences[32]), sentiment[32]"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('@ IndiaToday Teri kimat dokodi ki ho gayi ... amit shah will capture telegana soon ... kcr will resign ...',\n",
              " 'negative')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLeITOEQ_8A6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cb799967-cdd4-431e-89f1-70ab3cc2a7ce"
      },
      "source": [
        "\" \".join(translated_sentences[32])"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'@ IndiaToday ‡§ü‡•á‡§∞‡§ø ‡§ï‡§ø‡§Æ‡§§‡•ç ‡§¶‡•ã‡§ï‡•ã‡§¶‡§ø ‡§ï‡§ø ‡§π‡•ã ‡§ó‡§Ø‡§ø ... ‡§Ö‡§Æ‡§ø‡§§‡•ç ‡§∂‡§π‡•ç will capture telegana soon ... kcr will resign ...'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdxL0woM__2K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8bfb9a61-0db6-48ae-e7fa-69e41e08fed2"
      },
      "source": [
        "preprocess_data(translated_sentences[32])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'‡§ü‡•á‡§∞‡§ø ‡§ï‡§ø‡§Æ‡§§‡•ç ‡§¶‡•ã‡§ï‡•ã‡§¶‡§ø ‡§ï‡§ø ‡§π‡•ã ‡§ó‡§Ø‡§ø ... ‡§Ö‡§Æ‡§ø‡§§‡•ç ‡§∂‡§π‡•ç will capture telegana soon ... kcr will resign ...'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvNwk7WKqW9j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoding = tokenizer.encode_plus(\n",
        "  preprocess_data(translated_sentences[32]),\n",
        "  max_length=60,\n",
        "  add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
        "  return_token_type_ids=False,\n",
        "  truncation=True,\n",
        "  pad_to_max_length=True,\n",
        "  return_attention_mask=True,\n",
        "  return_tensors='pt',  # Return PyTorch tensors\n",
        ")"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYvPpI2aqml2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        },
        "outputId": "0b763090-3277-438d-e0ab-a0df2e35a00c"
      },
      "source": [
        "print(len(encoding['input_ids'][0]))\n",
        "encoding['input_ids'][0]"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([     0,  46005,  18992,   1682, 154156,  10850,    356,  13551,   1682,\n",
              "          1253,   5167,  67625,    153, 129069,   4377,   8933,   3849,   4377,\n",
              "          1221, 141621,   5501,  24869,  33662,    153,    472,  23150,   1221,\n",
              "        199747,    153,      2,      1,      1,      1,      1,      1,      1,\n",
              "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
              "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
              "             1,      1,      1,      1,      1,      1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kz-m4mcDqrZt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "3309dfef-c542-4a63-81d3-f860ff718ab4"
      },
      "source": [
        "print(len(encoding['attention_mask'][0]))\n",
        "encoding['attention_mask']"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDA5DkOjrO5n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b93597ff-e4b9-4260-9de5-521e3d8ab7b5"
      },
      "source": [
        "tokenizer.convert_ids_to_tokens(encoding['input_ids'][0])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<s>',\n",
              " '‚ñÅ',\n",
              " '‡§É',\n",
              " '‡§Ü',\n",
              " '‡§®‡•ç',\n",
              " '‚ñÅ‡§Ø‡§æ',\n",
              " '‡§∞‡•ç',\n",
              " '‚ñÅ‡§®‡•á',\n",
              " '‡§π',\n",
              " '‚ñÅ',\n",
              " 'üòî',\n",
              " 'üòî',\n",
              " '‚ñÅ‡§ï',\n",
              " '‡§¨‡•ç',\n",
              " '‚ñÅ‡§ï‡§∞‡•á',\n",
              " '‡§ó',\n",
              " '‚ñÅ‡§µ‡•ã',\n",
              " '‡§π',\n",
              " '‡•ç',\n",
              " '‚ñÅ‡§™‡•ã',\n",
              " '‡§∏‡•ç‡§§',\n",
              " '‡•ç',\n",
              " '‚ñÅ',\n",
              " 'üò≠',\n",
              " '‚ñÅ‡§ä',\n",
              " '‡§∏‡•ç',\n",
              " '‡§®‡•á',\n",
              " '‚ñÅ‡§®',\n",
              " '‚ñÅ‡§∏',\n",
              " '‡§ö‡•ç',\n",
              " '‚ñÅ‡§Æ‡•á',\n",
              " '‡§á‡§®‡•ç',\n",
              " '‚ñÅphoto',\n",
              " 'shoot',\n",
              " '‚ñÅ‡§ï‡§∞',\n",
              " '‡•ç‡§®',\n",
              " '‚ñÅ‡§ö',\n",
              " '‡§π',\n",
              " '‡§ø‡§Ø‡•á',\n",
              " '‚ñÅ‡§´‡§ø‡§∞',\n",
              " '‡•ç',\n",
              " '‚ñÅ‡§µ‡•ã',\n",
              " '‡§π',\n",
              " '‡•ç',\n",
              " '‚ñÅ‡§™‡•ã',\n",
              " '‡§∏‡•ç‡§§',\n",
              " '‡•ç',\n",
              " '‚ñÅ‡§ï‡§∞‡•á',\n",
              " '‡§ó',\n",
              " '</s>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cu27ifEeAG6i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "e114dc89-05dc-4719-9415-cad077f27616"
      },
      "source": [
        "\" \".join(sentences[29]), sentiment[29]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Madam @ SushmaSwaraj ji we always miss you as a # videsh _ mantri',\n",
              " 'positive')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbhxDrXLAcef",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9602d9c5-9577-421c-f1bd-6329bf6cf450"
      },
      "source": [
        "\" \".join(translated_sentences[29])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'‡§Ç‡§Ö‡§¶‡§Æ‡•ç @ S‡§â‡§∂‡•ç‡§ÆS‡§µ‡§∞‡§ú‡•ç ‡§ú‡§ø we always miss you as a # ‡§µ‡§ø‡§¶‡•á‡§∂‡•ç _ ‡§Æ‡§®‡•ç‡§§‡•ç‡§∞‡§ø'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgTwgMcNAi1L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "78f6009e-90a5-4e2e-9c1b-901e1ebd4db6"
      },
      "source": [
        "preprocess_data(translated_sentences[29])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'‡§Ç‡§Ö‡§¶‡§Æ‡•ç ‡§ú‡§ø we always miss you as a # ‡§µ‡§ø‡§¶‡•á‡§∂‡•ç _ ‡§Æ‡§®‡•ç‡§§‡•ç‡§∞‡§ø'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVD50xbdAqLt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "71e11fad-e9cb-4643-96c8-66b404042192"
      },
      "source": [
        "\" \".join(sentences[10]), sentiment[10]"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('@ ECISVEEP Can you answer miscalculated votes on each seat ? One vote matters ! # deshkamahatyohar hai aur apne dhji ‚Ä¶ https // t . co / SuHS4mx6Dm',\n",
              " 'neutral')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPo3jSKUA90I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2d8e8138-6b6d-43c4-a25e-64e5cfac5b87"
      },
      "source": [
        "\" \".join(translated_sentences[10])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'@ ECISVEEP Can you answer miscalculated votes on each seat ? One vote ‡§Æ‡§§‡•ç‡§§‡•á‡§∞‡•ç‡§∏‡•ç ! # ‡§¶‡•á‡§∂‡•ç‡§ï‡§Æ‡§π‡§§‡•ç‡§Ø‡•ã‡§π‡§∞‡•ç ‡§π‡•à ‡§î‡§∞‡•ç ‡§Ö‡§™‡•ç‡§®‡•á dhji ‚Ä¶ https // t . c‡§ì / S‡§â‡§ÉS‡•™‡§Æ‡•ç‡§ï‡•ç‡§∑‡•ç‡•¨‡§°‡•ç‡§Æ‡•ç'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PV43kaThA_w6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "733de627-0ffa-4182-89bf-a91e41d4b65c"
      },
      "source": [
        "preprocess_data(translated_sentences[10])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Can you answer miscalculated votes on each seat ? One vote ‡§Æ‡§§‡•ç‡§§‡•á‡§∞‡•ç‡§∏‡•ç ! # ‡§¶‡•á‡§∂‡•ç‡§ï‡§Æ‡§π‡§§‡•ç‡§Ø‡•ã‡§π‡§∞‡•ç ‡§π‡•à ‡§î‡§∞‡•ç ‡§Ö‡§™‡•ç‡§®‡•á dhji'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dz7VUX61L2hV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "86b1dbcb-8ce2-4100-b8e7-cfbe10779c05"
      },
      "source": [
        "%%time\n",
        "processed_sentences = []\n",
        "\n",
        "for sent in translated_sentences:\n",
        "    processed_sentences.append(preprocess_data(sent))\n",
        "\n",
        "test_data = []\n",
        "\n",
        "for sent in test_translated_sentences:\n",
        "    test_data.append(preprocess_data(sent))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 660 ms, sys: 4.4 ms, total: 664 ms\n",
            "Wall time: 665 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiSguBUPPUff",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentiment_mapping = {\n",
        "    \"negative\": 0,\n",
        "    \"neutral\": 1,\n",
        "    \"positive\": 2\n",
        "}"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5CXiR84PYdw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = [sentiment_mapping[sent] for sent in sentiment]\n",
        "test_label = [sentiment_mapping[sent] for sent in test_sentiment]"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SZj6jOJPbVa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_uids, val_uids, train_data, val_data, train_label, val_label = train_test_split(uids, processed_sentences, labels, test_size=0.2)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQI8f9HQPeLw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2e02d42d-f189-4c9a-dc5b-e31202a2bec7"
      },
      "source": [
        "len(train_data), len(val_data), len(test_data)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11200, 2800, 3000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RbS_7uFRsP0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_token_lengths = [len(sent.split()) for sent in train_data]"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXXvHvoBRDDV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LEN = 150"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNdcljj1ikJc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SentiMixDataSet(Dataset):\n",
        "    def __init__(self, inputs, labels, tokenizer, max_len):\n",
        "        self.sentences = inputs\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "    \n",
        "    def __getitem__(self, item):\n",
        "        sentence = self.sentences[item]\n",
        "        sentiment = int(self.labels[item])\n",
        "        \n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            sentence,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            pad_to_max_length=True,\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        \n",
        "        return {\n",
        "            \"text\": sentence,\n",
        "            \"input_ids\": encoding['input_ids'].flatten(),\n",
        "            \"attention_mask\": encoding['attention_mask'].flatten(),\n",
        "            \"label\": torch.tensor(sentiment, dtype=torch.long)\n",
        "        }"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmQjjQvUkgk6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = SentiMixDataSet(train_data, train_label, tokenizer, MAX_LEN)\n",
        "val_dataset = SentiMixDataSet(val_data, val_label, tokenizer, MAX_LEN)\n",
        "test_dataset = SentiMixDataSet(test_data, test_label, tokenizer, MAX_LEN)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2y5QX1cLkJOT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pT5I4IrhAu3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_loader = DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE)\n",
        "valid_data_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "test_data_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAImtABcKs9y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sample\n",
        "sample = next(iter(train_data_loader))"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEhCZVtWKzSc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d539b9f5-5a3b-4975-db93-a7e0a288cfbc"
      },
      "source": [
        "sample[\"input_ids\"].shape, sample[\"attention_mask\"].shape, sample[\"label\"].shape"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([64, 150]), torch.Size([64, 150]), torch.Size([64]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qy72xb5uk7f0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class XLMModel(nn.Module):\n",
        "    def __init__(self, output_dim, dropout=0.3):\n",
        "        super().__init__()\n",
        "\n",
        "        self.bert = XLMRobertaModel.from_pretrained(MODEL_NAME)\n",
        "        hidden_size = self.bert.config.to_dict()['hidden_size']\n",
        "\n",
        "        self.out = nn.Linear(hidden_size, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # text => [batch_size, seq_len]\n",
        "\n",
        "        _, pooled_output = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "        logits = self.out(self.dropout(pooled_output))\n",
        "        return logits"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEGZN1PnoraP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output_dim = 3\n",
        "model = XLMModel(output_dim)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qx-gtJZ2pBJF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = model.to(device)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OnP9qivpEB8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e1883091-d9c8-4abf-f6e1-e0f2df7e65a2"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 278,045,955 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-VtXUd2pOId",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 10\n",
        "lr = 5e-5\n",
        "min_lr = 1e-7\n",
        "lr_decay=0.5\n",
        "lr_patience=2\n",
        "\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "scheduler = ReduceLROnPlateau(optimizer, 'min', lr_decay, lr_patience, verbose=True, min_lr=min_lr)\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "di9_xMiXq1zy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, iterator, clip=2.0):\n",
        "    epoch_loss = 0\n",
        "    model.train()\n",
        "\n",
        "    for batch in iterator:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        targets = batch[\"label\"].to(device)\n",
        "\n",
        "        predictions = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_fn(predictions, targets)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJP8yQfJ4ygz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def simple_accuracy(preds, labels):\n",
        "    \"\"\"Takes in two lists of predicted labels and actual labels and returns the accuracy in the form of a float. \"\"\"\n",
        "    return np.equal(preds, labels).mean()"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5Eue3S12lGh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, iterator):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    preds = []\n",
        "    trgs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            targets = batch[\"label\"].to(device)\n",
        "\n",
        "            predictions = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask\n",
        "            )\n",
        "            \n",
        "            loss = loss_fn(predictions, targets)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            trgs.extend(targets.detach().cpu().numpy().tolist())\n",
        "            _, predicted = torch.max(predictions, 1)\n",
        "            preds.extend(predicted.detach().cpu().numpy().tolist())\n",
        "\n",
        "    return epoch_loss / len(iterator), simple_accuracy(preds, trgs)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uLTjMTM3zQK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29dDAQCv3z3P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "outputId": "4eef33dc-f020-4ddc-ef66-feb02c2524e9"
      },
      "source": [
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start_time = time.time()\n",
        "    train_loss = train(model, train_data_loader)\n",
        "    val_loss, val_acc = evaluate(model, valid_data_loader)\n",
        "    end_time = time.time()\n",
        "    scheduler.step(val_loss)\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    print(f\"Epoch: {epoch + 1:02} | Time: {epoch_mins}m {epoch_secs:.2f}s\")\n",
        "    print(f\"\\tTrain Loss: {train_loss:.3f} | Val Loss: {val_loss:.3f} | Val Acc: {val_acc:.3f}\")\n",
        "    \n",
        "    if val_loss < best_valid_loss:\n",
        "        best_valid_loss = val_loss\n",
        "        torch.save(model.state_dict(), 'xlm_roberta.pt')\n"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 5m 17.00s\n",
            "\tTrain Loss: 1.007 | Val Loss: 0.932 | Val Acc: 0.558\n",
            "Epoch: 02 | Time: 5m 25.00s\n",
            "\tTrain Loss: 0.896 | Val Loss: 0.954 | Val Acc: 0.551\n",
            "Epoch: 03 | Time: 5m 25.00s\n",
            "\tTrain Loss: 0.832 | Val Loss: 0.882 | Val Acc: 0.594\n",
            "Epoch: 04 | Time: 5m 25.00s\n",
            "\tTrain Loss: 0.758 | Val Loss: 0.916 | Val Acc: 0.587\n",
            "Epoch: 05 | Time: 5m 25.00s\n",
            "\tTrain Loss: 0.681 | Val Loss: 0.943 | Val Acc: 0.596\n",
            "Epoch: 06 | Time: 5m 25.00s\n",
            "\tTrain Loss: 0.596 | Val Loss: 1.002 | Val Acc: 0.599\n",
            "Epoch: 07 | Time: 5m 25.00s\n",
            "\tTrain Loss: 0.517 | Val Loss: 1.116 | Val Acc: 0.585\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-00dd3313984b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_data_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-43-577eee8cbb0a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, clip)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmEpM-NKLz5A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "outputId": "73c07712-8752-4c8b-8ad5-df42dc73e9b9"
      },
      "source": [
        "model.load_state_dict(torch.load('xlm_roberta.pt'))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-2c9822c8921b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'xlm_roberta.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ai-_jVYSPjN-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "ad596641-87e7-4a08-e875-6ad9643de545"
      },
      "source": [
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    targets = []\n",
        "    for batch in test_data_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        targets = batch[\"label\"].to(device)\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "        # get the predicted labels\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        # Add data to lists\n",
        "        preds.extend(predicted.detach().cpu().numpy().tolist())\n",
        "        targets.extend(batch.label.detach().cpu().numpy().tolist())\n",
        "\n",
        "print(metrics.classification_report(targets, preds))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e8e63f0ae8ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_data_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zlVS3CORh0V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}